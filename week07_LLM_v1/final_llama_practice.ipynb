{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8373863c",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA Tutorial with Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21a994",
   "metadata": {},
   "source": [
    "\n",
    "In this practical seminar, we will go through the full pipeline for fine-tuning the LLaMA model.\n",
    "Each section includes theoretical context and links to documentation to ensure a comprehensive understanding \n",
    "of the implementation. **Note**: Ensure you have installed all required packages as listed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998af1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec82bf5",
   "metadata": {},
   "source": [
    "\n",
    "**Theory**: We begin by installing essential packages, including `transformers` for the model, `datasets` for handling our dataset, \n",
    "and `accelerate` to efficiently distribute computations. `bitsandbytes` allows for lower-precision quantization, optimizing performance.\n",
    "Refer to [Transformers documentation](https://huggingface.co/docs/transformers/index) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd881f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c8736",
   "metadata": {},
   "source": [
    "### Checking GPU Availability\n",
    "`nvidia-smi` command is used to verify GPU status and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01899fba",
   "metadata": {},
   "source": [
    "\n",
    "### Importing Required Libraries\n",
    "\n",
    "- `AutoModelForCausalLM` and `AutoTokenizer` from Hugging Face's `transformers` for loading a pre-trained language model.\n",
    "- `Dataset` from `datasets` for creating and managing data efficiently.\n",
    "- `Trainer` and `TrainingArguments` for configuring and training the model.\n",
    "- `torch` for handling tensor operations.\n",
    "- `logging` for enabling info-level logging to track progress.\n",
    "\n",
    "For more on each of these, refer to the [datasets library](https://huggingface.co/docs/datasets/) and [transformers library](https://huggingface.co/docs/transformers/) documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_dataset(size, digits=(2, 18), operation='addition'):\n",
    "    logger.info(\"Generating dataset with varied difficulty.\")\n",
    "    for _ in range(size):\n",
    "        a = random.randint(10**digits[0], 10**digits[1])\n",
    "        b = random.randint(10**digits[0], 10**digits[1])\n",
    "        if operation == 'addition':\n",
    "            c = a + b\n",
    "            prompt = f'Calculate the sum of {a} and {b}: {c}'\n",
    "        elif operation == 'multiplication':\n",
    "            c = a * b\n",
    "            prompt = f'Calculate the product of {a} and {b}: {c}'\n",
    "        yield {'prompt': prompt, 'response': str(c)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d91a12",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset Generation Function\n",
    "\n",
    "This function generates a dataset of simple arithmetic problems, with each example containing a prompt (arithmetic question) \n",
    "and the corresponding answer. Here, we're generating synthetic data for fine-tuning purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4060a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = Dataset.from_generator(gen_dataset, gen_kwargs={\"size\": 400, \"digits\": (2, 18), \"operation\": \"addition\"})\n",
    "test_dataset = Dataset.from_generator(gen_dataset, gen_kwargs={\"size\": 40, \"digits\": (8, 10), \"operation\": \"multiplication\"})\n",
    "logger.info(f\"Generated train dataset size: {len(train_dataset)}, test dataset size: {len(test_dataset)}\")\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13164565",
   "metadata": {},
   "source": [
    "\n",
    "### Creating Train and Test Datasets\n",
    "\n",
    "Using `from_generator`, we generate two datasets (train and test) for addition and multiplication operations. \n",
    "The [datasets.from_generator](https://huggingface.co/docs/datasets/v2.1.0/en/package_reference/main_classes#datasets.Dataset.from_generator) function creates a `Dataset` from a generator function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ce55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567b504",
   "metadata": {},
   "source": [
    "\n",
    "### Logging into Hugging Face Hub\n",
    "\n",
    "This cell logs into the Hugging Face Hub for accessing pretrained models and saving fine-tuned models. \n",
    "See [huggingface_hub login documentation](https://huggingface.co/docs/huggingface_hub/quick_start#step-3-log-in-to-the-hugging-face-hub).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a728ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc767d",
   "metadata": {},
   "source": [
    "\n",
    "### Configuring BitsAndBytes Quantization\n",
    "\n",
    "We configure BitsAndBytes for 4-bit quantization to reduce memory usage, making model fine-tuning more efficient. \n",
    "Read more about [BitsAndBytesConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.BitsAndBytesConfig).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\", add_eos_token=True, add_bos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "logger.info(f\"Model {model_name} loaded with {model.num_parameters()} parameters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e5e92",
   "metadata": {},
   "source": [
    "\n",
    "### Loading the Pre-trained Model and Tokenizer\n",
    "\n",
    "We initialize the LLaMA model and tokenizer, setting `add_eos_token` and `add_bos_token` for proper tokenization handling.\n",
    "The `device_map=\"auto\"` automatically distributes model parts across available devices, improving efficiency. \n",
    "For more, see [AutoTokenizer documentation](https://huggingface.co/docs/transformers/main_classes/tokenizer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b41bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_data(prompt):\n",
    "    return tokenizer(prompt['prompt'])\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_data)\n",
    "test_dataset = test_dataset.map(tokenize_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81907b8",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenizing the Dataset\n",
    "\n",
    "This function tokenizes each prompt from the dataset, enabling the model to process them effectively. \n",
    "The `map` function applies the tokenization across all dataset examples. For more, see [map documentation](https://huggingface.co/docs/datasets/process#map).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(train_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b359ff3",
   "metadata": {},
   "source": [
    "\n",
    "### Plotting Data Lengths\n",
    "\n",
    "This section provides a histogram of tokenized prompt lengths, visualizing the distribution of input lengths.\n",
    "This can help in setting max length parameters for padding/truncation during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 32\n",
    "\n",
    "def tokenize_data(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt['prompt'],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_data)\n",
    "test_dataset = test_dataset.map(tokenize_data)\n",
    "logger.info(\"Tokenization complete with diagnostic shape checks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24802608",
   "metadata": {},
   "source": [
    "\n",
    "### Applying Padding and Label Creation\n",
    "\n",
    "This code modifies tokenization to include truncation, padding, and copying input IDs to labels, which is necessary \n",
    "for certain types of language model training. See [padding and truncation documentation](https://huggingface.co/docs/transformers/padding_and_truncation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b64faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a49f4a",
   "metadata": {},
   "source": [
    "\n",
    "### Preparing Model for Efficient Training with K-Bit Precision\n",
    "\n",
    "This section enables gradient checkpointing, reducing memory usage during training. \n",
    "For more, see [gradient checkpointing](https://huggingface.co/docs/transformers/main_classes/accelerate#transformers.Accelerate.gradient_checkpointing_enable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbf314",
   "metadata": {},
   "source": [
    "\n",
    "### Printing Trainable Parameters\n",
    "\n",
    "This function calculates and displays the number of trainable parameters in the model, a critical metric for efficient fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860579a",
   "metadata": {},
   "source": [
    "\n",
    "### Configuring and Applying LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) configuration reduces memory and computational requirements, focusing on specific layers. \n",
    "For more, check [LoRA documentation](https://huggingface.co/docs/peft/api/lora).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "logger.info(\"Training configuration set with advanced parameters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab402f",
   "metadata": {},
   "source": [
    "\n",
    "### Setting Training Arguments\n",
    "\n",
    "Here we define `TrainingArguments` for fine-tuning, such as batch size, learning rate, and logging. \n",
    "For more details, refer to [TrainingArguments documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b551c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "logger.info(\"Trainer initialized with training and evaluation datasets.\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8c6f0",
   "metadata": {},
   "source": [
    "\n",
    "### Initializing and Training the Model\n",
    "\n",
    "The `Trainer` class simplifies model training, managing the training loop, evaluation, and logging.\n",
    "Check the [Trainer documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316803c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_results = trainer.evaluate()\n",
    "logger.info(f\"Evaluation Results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4dd7c6",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Evaluates the fine-tuned model using the test dataset and logs results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    example = test_dataset[i]\n",
    "    input_ids = tokenizer(example['prompt'], return_tensors=\"pt\").input_ids\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=32)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {example['prompt']})\n",
    "Expected Output: {example['response']}\n",
    "Model Output: {output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c2f00",
   "metadata": {},
   "source": [
    "\n",
    "### Testing Model Outputs\n",
    "\n",
    "Generates outputs for the test set to compare model predictions with expected responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f112b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_inputs = [\"Calculate the sum of 10 and 15:\", \"Calculate the sum of 4 and 6:\"]\n",
    "for input_text in sample_inputs:\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    output_ids = model.generate(input_ids)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {input_text}\n",
    "Output: {output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f47ee6",
   "metadata": {},
   "source": [
    "\n",
    "### Additional Test Cases\n",
    "\n",
    "Here, we input new arithmetic prompts to see how the model generalizes to similar tasks beyond the test dataset.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
