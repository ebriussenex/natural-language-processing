{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "1SHvpiBWHOzA",
    "p1LMii_5IAlc",
    "OFxP7ywasE-A"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Flash Attention"
   ],
   "metadata": {
    "id": "y3e1uulnGDGX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Flash Attention is an optimized attention mechanism that addresses the issues of the classical attention mechanism, such as high memory requirements and inefficient GPU utilization. This notebook (made by [n.luneva](https://github.com/lwtztea)) will help you understand how Flash Attention works and demonstrate its practical applications.\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2205.14135) and [HF explanation](https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention)."
   ],
   "metadata": {
    "id": "Bk1Kf12aQFEZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1. Theory"
   ],
   "metadata": {
    "id": "CmJZUio6F1YP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is the Attention Mechanism?"
   ],
   "metadata": {
    "id": "1SHvpiBWHOzA",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Attention mechanism is used in transformer architectures to compute relationships between elements in a sequence. The classical attention mechanism requires calculating an attention scores matrix of size $n \\times n$, which leads to the following problems:\n",
    "\n",
    "1. **High memory requirements**: $O(n^2)$.\n",
    "2. **Inefficient GPU Utilization.** Large matrices do not fit well in GPU memory.\n",
    "3. **Scaling challenges.** Execution time grows quadratically with increasing sequence length."
   ],
   "metadata": {
    "id": "6gz9JQDwQbX5",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is Flash Attention?"
   ],
   "metadata": {
    "id": "p1LMii_5IAlc",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Flash Attention is an optimized implementation of the attention mechanism that:\n",
    "\n",
    "- Reduces memory consumption from $O(n^2)$ to $O(n)$.\n",
    "- Uses streaming computation and tiling for efficient operation on GPUs.\n",
    "- Ensures numerical stability when computing softmax."
   ],
   "metadata": {
    "id": "CWb7KpVqQdPn",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = https://raw.githubusercontent.com/lwtztea/ml_pic/269a45a/week_6/flash_attention.png width = 2000 >"
   ],
   "metadata": {
    "id": "JEx2X8tNVK14",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A reason why we need to split inputs into blocks is that SRAM does not have enough space to save full intermediate results in it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = https://raw.githubusercontent.com/lwtztea/ml_pic/957bf7b/week_6/memory_hierarchy.png width = 1000 >"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = https://raw.githubusercontent.com/lwtztea/ml_pic/957bf7b/week_6/fa_algorithm.png width = 2000 >"
   ],
   "metadata": {
    "id": "ENK8euT1QEqS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can check computation animation on YouTube:\n",
    "\n",
    "* Standard Attention — [link](https://www.youtube.com/watch?v=-EF-KIscwJw&list=PLBWdTDczuFNrxnetcH-5CVLRP1u4Ua87m&index=1)\n",
    "* Flash Attention — [link](https://www.youtube.com/watch?v=cq3jQ-Bbmzs&list=PLBWdTDczuFNrxnetcH-5CVLRP1u4Ua87m&index=5)\n",
    "\n",
    "A little about block matrix multiplication — [link](https://mathworld.wolfram.com/BlockMatrix.html)."
   ],
   "metadata": {
    "id": "GcOn49SDlVam",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Another Secret — Fused Kernel"
   ],
   "metadata": {
    "id": "OFxP7ywasE-A",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fused Kernel is an optimization technique in which multiple computational operations are combined into a single \"kernel\" computation at the GPU level. This minimizes overhead associated with moving data between different memory levels (e.g., between GPU global memory and registers) and reduces the number of kernel launches."
   ],
   "metadata": {
    "id": "ub-4q25As6wR",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a traditional approach, each operation (e.g., matrix multiplication, softmax or scaling) is executed separately, leading to the following issues:\n",
    "\n",
    "* **Frequent memory accesses.** Intermediate results of each operation are written to GPU global memory and then read back for the next operation.\n",
    "* **Multiple kernel launches.** Each operation requires a separate kernel launch, which increases latency.\n",
    "* **Inefficient use of GPU resources.** GPU global memory is slower than local memory or registers, so frequent accesses to it reduce performance."
   ],
   "metadata": {
    "id": "ALPI2QHptolo",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fused Kernel in the Context of Flash Attention"
   ],
   "metadata": {
    "id": "IYpEKEszugRx",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Flash Attention actively uses fused kernels to combine multiple stages of the attention mechanism into a single computation.\n",
    "\n",
    "**Traditional Approach:**\n",
    "1. Compute attention scores: $S = QK^T$.\n",
    "2. Store $S$ in global memory.\n",
    "3. Apply scaling $S' = S / \\sqrt{d_k}$.\n",
    "4. Apply softmax: $A = \\text{softmax}(S')$.\n",
    "5. Store $A$ in global memory.\n",
    "6. Compute the output: $O = AV$.\n",
    "\n",
    "Each step requires access to global memory and a separate kernel launch.\n",
    "\n",
    "**Fused Kernel Approach:**\n",
    "1. Compute attention scores, scaling and softmax in a single kernel, storing only intermediate results in registers.\n",
    "2. Directly compute the output $O = AV$.\n",
    "\n",
    "Fused kernels are usually implemented in low-level programming languages such as CUDA for maximum optimization. However, modern libraries like PyTorch and TensorFlow provide high-level interfaces for working with it."
   ],
   "metadata": {
    "id": "jWX_WiTzO_hT",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = https://raw.githubusercontent.com/lwtztea/ml_pic/957bf7b/week_6/fused_kernel.png width = 1000 >"
   ],
   "metadata": {
    "id": "9fdHXlWPsXDB",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Implementation\n",
    "\n",
    "In this section, we will look at an example implementation of Flash Attention in Python using the PyTorch library."
   ],
   "metadata": {
    "id": "5wYhuGipGLGN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "id": "DvMZpYT1RBP_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dxcMhrwY1ddk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def flash_attention(Q, K, V, block_size=64):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q (torch.Tensor): Query [batch_size, seq_len, d_k].\n",
    "        K (torch.Tensor): Key [batch_size, seq_len, d_k].\n",
    "        V (torch.Tensor): Value [batch_size, seq_len, d_v].\n",
    "        block_size (int): tiling block size.\n",
    "    Returns:\n",
    "        torch.Tensor: Attention output.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_k = Q.shape\n",
    "    device = Q.device\n",
    "\n",
    "    O = torch.zeros_like(V)\n",
    "    l_i = torch.zeros(batch_size, seq_len, device=device)\n",
    "    m_i = torch.full((batch_size, seq_len), float(\"-inf\"), device=device)\n",
    "\n",
    "    for start in range(0, seq_len, block_size):\n",
    "        end = min(start + block_size, seq_len)\n",
    "\n",
    "        Q_block = Q[:, start:end, :]\n",
    "        S_block = torch.matmul(Q_block, K.transpose(-2, -1)) / (d_k**0.5)\n",
    "\n",
    "        M_block = torch.max(S_block, dim=-1, keepdim=True).values\n",
    "        P_block = torch.exp(S_block - M_block)\n",
    "        L_block = torch.sum(P_block, dim=-1, keepdim=True)\n",
    "\n",
    "        O[:, start:end, :] = torch.matmul(P_block / L_block, V)\n",
    "\n",
    "        l_i[:, start:end] = L_block.squeeze(-1)\n",
    "        m_i[:, start:end] = M_block.squeeze(-1)\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size, seq_len, d_k = 2, 10, 8\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output = flash_attention(Q, K, V)\n",
    "print(\"Attention output:\")\n",
    "print(output)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsM3Mr3ARNEY",
    "outputId": "65393a37-085c-46ad-b2c6-afa1cbc1789d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output:\n",
      "tensor([[[ 5.7712e-01,  7.4143e-02,  9.4435e-01, -6.2278e-01, -7.9697e-01,\n",
      "          -7.3283e-01,  6.3864e-01,  1.5707e-01],\n",
      "         [ 3.8677e-01, -1.3144e-01,  8.9284e-01, -7.3510e-01, -3.9344e-01,\n",
      "          -2.4009e-01,  3.3335e-01, -9.2881e-02],\n",
      "         [ 1.8090e-01, -2.2231e-01,  6.1934e-01, -7.4626e-01,  8.5221e-02,\n",
      "          -3.1526e-01,  1.2311e-01,  2.9450e-01],\n",
      "         [ 2.8106e-02, -1.8882e-01,  3.4479e-01, -5.2488e-01,  9.6422e-02,\n",
      "          -3.0207e-01,  7.6238e-02,  2.8863e-01],\n",
      "         [-2.1090e-01, -6.2028e-01, -6.0037e-01, -2.6804e-01,  1.0242e+00,\n",
      "          -3.3890e-02,  3.3172e-02,  6.0404e-01],\n",
      "         [ 1.0859e-01, -1.8317e-02,  5.5349e-01, -6.8297e-01, -2.4812e-02,\n",
      "          -1.1835e-01,  7.0378e-02, -6.5696e-02],\n",
      "         [ 4.4700e-01, -6.3494e-01,  1.5653e-01, -7.0236e-01, -6.4877e-02,\n",
      "           4.9781e-02, -6.6919e-02,  4.3952e-02],\n",
      "         [ 3.0520e-01, -2.5359e-01,  6.6639e-01, -6.1226e-01, -4.4767e-01,\n",
      "          -1.7331e-01,  1.8060e-01,  7.6832e-02],\n",
      "         [ 6.5281e-01, -3.7353e-01,  7.6377e-01, -7.0979e-01, -4.3452e-01,\n",
      "          -4.1592e-01,  2.9523e-01,  2.4506e-02],\n",
      "         [ 8.4860e-03, -9.8949e-03,  6.0883e-01, -5.0481e-01, -2.5019e-01,\n",
      "          -9.1597e-02,  4.5539e-02,  2.0131e-01]],\n",
      "\n",
      "        [[-2.7610e-01, -1.1889e+00,  6.8779e-02, -4.2774e-03, -1.9594e-01,\n",
      "           9.2401e-02, -2.9112e-01,  7.6360e-02],\n",
      "         [-3.4467e-01, -3.1808e-01,  8.7856e-02, -2.5937e-01,  2.4828e-01,\n",
      "           6.5988e-02, -6.9172e-01,  1.9127e-01],\n",
      "         [-3.0163e-01,  5.1396e-01, -2.7782e-01, -1.2004e-01,  3.9018e-01,\n",
      "          -3.8210e-01, -8.9205e-01,  1.7672e-01],\n",
      "         [-3.1443e-01, -3.5332e-01, -9.3414e-02,  5.6455e-02,  8.9680e-02,\n",
      "          -1.2910e-01, -3.9515e-01, -1.5624e-01],\n",
      "         [-2.7409e-01,  5.4424e-02, -1.2838e-01, -6.9820e-03, -4.7626e-02,\n",
      "          -3.2715e-01, -5.6383e-01, -2.0176e-01],\n",
      "         [-3.5709e-01,  5.0801e-01,  5.3803e-02, -2.9480e-01,  3.9819e-01,\n",
      "          -1.3589e-01, -8.4207e-01, -4.1361e-02],\n",
      "         [-3.4125e-01,  4.3183e-01, -8.8149e-02, -2.7331e-01,  5.2540e-01,\n",
      "          -1.9230e-01, -9.6347e-01, -1.0912e-03],\n",
      "         [-1.8199e-01, -2.5281e-01, -9.9502e-02, -4.3989e-02, -1.0363e-01,\n",
      "          -3.9281e-01, -5.2577e-01, -1.6044e-02],\n",
      "         [-1.4535e-01, -1.0344e+00, -2.0918e-03, -3.1575e-02, -6.7723e-02,\n",
      "          -2.7357e-02, -4.1686e-01,  1.4201e-01],\n",
      "         [-3.9190e-01,  3.2293e-01, -2.0709e-01, -1.4349e-01,  6.0286e-01,\n",
      "          -1.2349e-01, -7.7717e-01,  2.2848e-01]]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Application\n",
    "\n",
    "Let's apply Flash Attention in the context of an NLP task. We'll create a simple transformer model with Flash Attention which will be used as an attention layer."
   ],
   "metadata": {
    "id": "rVpuGBaFF9Mx",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerWithFlashAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        Q = Q.reshape(batch_size * self.num_heads, seq_len, self.d_k)\n",
    "        K = K.reshape(batch_size * self.num_heads, seq_len, self.d_k)\n",
    "        V = V.reshape(batch_size * self.num_heads, seq_len, self.d_k)\n",
    "        output = flash_attention(Q, K, V)\n",
    "\n",
    "        output = output.view(batch_size, self.num_heads, seq_len, self.d_k)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        return self.W_o(output)"
   ],
   "metadata": {
    "id": "Mj4nhh1CF7ax",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = TransformerWithFlashAttention(d_model=128, num_heads=8)\n",
    "x = torch.randn(2, 10, 128)\n",
    "output = model(x)\n",
    "print(\"Model output:\")\n",
    "print(output)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tW-RONiwSH1b",
    "outputId": "ae2beeb0-7df4-4826-bca7-691793a514f2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output:\n",
      "tensor([[[-0.0225, -0.0889,  0.0936,  ...,  0.0526, -0.0591,  0.0948],\n",
      "         [-0.0018, -0.1090,  0.1823,  ...,  0.0269,  0.0462,  0.0804],\n",
      "         [ 0.0081, -0.0597,  0.1509,  ...,  0.0789, -0.0389,  0.1391],\n",
      "         ...,\n",
      "         [ 0.0415, -0.0422,  0.1358,  ...,  0.1021, -0.0331,  0.1239],\n",
      "         [-0.0240, -0.1373,  0.1252,  ...,  0.0315, -0.0362,  0.0909],\n",
      "         [-0.0701, -0.0948,  0.1976,  ...,  0.0267,  0.0213,  0.1033]],\n",
      "\n",
      "        [[ 0.0845, -0.1266, -0.0191,  ..., -0.0731, -0.0965,  0.2096],\n",
      "         [ 0.0781, -0.1326, -0.1218,  ..., -0.0705, -0.1542,  0.1669],\n",
      "         [ 0.0564, -0.1580, -0.0757,  ..., -0.0909, -0.1825,  0.1700],\n",
      "         ...,\n",
      "         [ 0.0809, -0.0882, -0.0278,  ..., -0.0616, -0.1072,  0.1951],\n",
      "         [ 0.0396, -0.1612, -0.0817,  ..., -0.1417, -0.0749,  0.2213],\n",
      "         [ 0.0315, -0.0974, -0.0551,  ..., -0.0613, -0.1030,  0.1355]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "Flash Attention is a powerful tool for improving the performance of transformer models. It enables efficient processing of long sequences and reduces memory requirements."
   ],
   "metadata": {
    "id": "qDtRnXTFGAlo",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}