{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "HkmMBpehThtt"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Key-value cache and Paged Attention"
   ],
   "metadata": {
    "id": "PC7TqhTist1O",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook made by [n.luneva](https://github.com/lwtztea)"
   ],
   "metadata": {
    "id": "twH_Wp8_bo49",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Key-Value Cache"
   ],
   "metadata": {
    "id": "M36lF2JUbMBQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Theory"
   ],
   "metadata": {
    "id": "HkmMBpehThtt",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://huggingface.co/docs/transformers/main/kv_cache\n",
    "\n",
    "Key-Value Cache is an optimization used in transformer models to speed up text generation in **autoregressive tasks**, such as machine translation, text generation or question answering. In these tasks, the model generates a sequence of tokens **one at the time**, where each new token depends on previously generated tokens."
   ],
   "metadata": {
    "id": "2pwK0OsDV3Yl",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uyuyOW1VBqmF5Gtv225XHQ.gif)"
   ],
   "metadata": {
    "id": "gvr4n869W1Vm",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Main Idea\n",
    "\n",
    "In the operation of a transformer, each layer uses the attention mechanism to compute representations of the current token based on the context from previous tokens. For this, the attention mechanism performs the following steps:\n",
    "\n",
    "1. **Compute Query.** A query is computed for the current token.\n",
    "2. **Compute Key and Value.** Keys and values are computed for all input tokens (including the current one).\n",
    "3. **Compute Attention.** Based on the query and keys, weights are calculated, which are then used for the weighted summation of values.\n",
    "\n",
    "During autoregressive text generation (token by token), many computations are repeated. For example, if the model has already processed the first 100 tokens, when generating the 101st token, it is unnecessary to recompute keys and values for the first 100 tokens, as they remain unchanged. This is where the Key-Value Cache comes into play."
   ],
   "metadata": {
    "id": "OTZoAERXUbjh",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### How Key-Value Cache Works?\n",
    "\n",
    "1. **Caching Keys and Values:**\n",
    "   - When the model processes the first token in the sequence, it computes keys and values for all available tokens (including the first one).\n",
    "   - These keys and values are stored in the cache.\n",
    "   - When processing the second token, the keys and values for the first token are retrieved from the cache, and only the keys and values for the second token are newly computed.\n",
    "   - This process continues for each new token: keys and values for previously processed tokens are retrieved from the cache, while those for the new token are computed anew.\n",
    "2. **Reducing Computational Load:**\n",
    "   - Without caching, at each generation step, the model would have to recompute keys and values for all previous tokens, leading to significant computational overhead.\n",
    "   - With caching, the amount of computation is greatly reduced since the keys and values for older tokens are not recomputed.\n",
    "3. **Memory Usage:**\n",
    "   - The cache is stored in memory, and its size grows linearly with the length of the input sequence.\n",
    "   - This requires additional memory but significantly speeds up the generation process."
   ],
   "metadata": {
    "id": "Pamlh-3TVFA2",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Advantages of Key-Value Cache\n",
    "\n",
    "1. **Faster Generation.** Caching avoids redundant computations of keys and values for already processed tokens, significantly speeding up the generation process.\n",
    "2. **Computational Resource Savings.** Reduces the number of matrix multiplication operations, which is especially important for large models with many parameters.\n",
    "3. **Scalability.** Suitable for handling long sequences, where repeated calculations would become prohibitively expensive."
   ],
   "metadata": {
    "id": "_NWEv9nbVcqh",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Drawbacks of Key-Value Cache\n",
    "\n",
    "1. **Memory Usage**:\n",
    "   - The cache requires additional memory to store keys and values for all processed tokens.\n",
    "   - The size of the cache grows linearly with the length of the input sequence, which can become an issue for very long sequences.\n",
    "2. **Limitations for Some Tasks**:\n",
    "   - If the input sequence changes dynamically (e.g., in streaming processing), managing the cache can become more complex."
   ],
   "metadata": {
    "id": "HLKF4gt7WGks",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Practice"
   ],
   "metadata": {
    "id": "DC3Rzg7VVoFs",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading Data and Model"
   ],
   "metadata": {
    "id": "MSrlDiWUbUZf",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "README.md: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7089b96f8f1c49b1a42acec1b641f366"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b75c2eb19cb48b2bce22a467d23a1e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e284a4db192d41e990b5ee484e5d612e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6ea6044edc2479fadf75911101c019c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4eca32f23da14fc0b9ce60e796738de1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36c9f68d0b024b0a8e462edd0c0efad8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a23de44bdc34e13b885806037b5207c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role\n"
     ]
    }
   ],
   "source": [
    "text = dataset[\"text\"][3]\n",
    "text = text[:100]  # get text prefix\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Implementation"
   ],
   "metadata": {
    "id": "7Osm0ThjtUKM",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class KeyValueCache:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.cache = {}\n",
    "\n",
    "    def generate_with_cache(self, input_ids, max_length=50):\n",
    "        past_key_values = None\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                past_key_values = outputs.past_key_values\n",
    "\n",
    "                next_token = torch.argmax(logits, dim=-1)\n",
    "                generated_tokens.append(next_token.item())\n",
    "\n",
    "                input_ids = next_token.unsqueeze(0)\n",
    "\n",
    "        return tokenizer.decode(generated_tokens), outputs"
   ],
   "metadata": {
    "id": "jwhVVS9Is8ua",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize kv-cache\n",
    "cache_model = KeyValueCache(model)\n",
    "\n",
    "# Generate using kv-cache\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "generated_text, outputs = cache_model.generate_with_cache(input_ids)\n",
    "\n",
    "print(f\"Input text: {text}\")\n",
    "print(f\"Generated text: {generated_text}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTMt4g6WtcGn",
    "outputId": "5943aad3-6205-4043-b591-8253549d1df0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:  Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role\n",
      "Generated text:  in the film, and he is a member of the cast of the film. He is also a member of the cast of the film.\n",
      "\n",
      "He is a member of the cast of the film, and he is a member of the cast of\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(outputs.past_key_values)  # attention blocks"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNzjGDELY3GH",
    "outputId": "a3518679-404a-468a-a08b-d9731b6710c6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(outputs.past_key_values[0])  # keys and values cache"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-L98c5RZVsq",
    "outputId": "fed09cb7-75fd-4803-df56-f5ea7a89b3f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "outputs.past_key_values[0][0].shape  # [batch_size, num_heads, seq_len, head_dim]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8Y7AddjlfLQ",
    "outputId": "4ba16b0b-ec08-4065-9f42-c9234b5edc51",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 12, 72, 64])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "outputs.past_key_values[0][1][0][0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLkLMPfSmaP9",
    "outputId": "8abb5e6a-64f2-4b28-b8b5-37754a981889",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0223,  0.0035,  0.0491,  ...,  0.0357,  0.0884,  0.0652],\n        [-0.0326, -0.1449,  0.0364,  ...,  0.1332,  0.0875,  0.0343],\n        [-0.0741, -0.1271, -0.0747,  ..., -0.2468,  0.0759,  0.0554],\n        ...,\n        [-0.0129, -0.1281, -0.0717,  ..., -0.0452, -0.0520,  0.2178],\n        [-0.1865, -0.0098,  0.1319,  ...,  0.0968,  0.0990, -0.1065],\n        [ 0.4145,  0.1236,  0.0328,  ..., -0.1359, -0.1635, -0.0631]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paged attention"
   ],
   "metadata": {
    "id": "_Rh57lmFuY0-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Theory"
   ],
   "metadata": {
    "id": "wxiWZzfJb93h",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://blog.vllm.ai/2023/06/20/vllm.html\n"
   ],
   "metadata": {
    "id": "KzgphszUcCh0",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = https://raw.githubusercontent.com/lwtztea/ml_pic/957bf7b/week_6/memory_wastes.png width = 2000 >"
   ],
   "metadata": {
    "id": "qbATC6SHjrUL",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://blog.vllm.ai/assets/figures/annimation1.gif)"
   ],
   "metadata": {
    "id": "upRvlYYNhMNV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Problem — Managing Attention in Transformers\n",
    "\n",
    "In traditional transformer architectures, the attention mechanism requires storing and processing keys and values for each position in the sequence. For a sequence of length N, this leads to the creation of matrices of size O(N×d), where d is the dimensionality of the hidden space. When N becomes very large (e.g., thousands or millions of tokens), the amount of memory required to store these matrices grows linearly, which can become overwhelming.\n",
    "\n",
    "Additionally, when using mechanisms like KV-caching (caching keys and values for reuse in autoregressive models), **memory quickly fills up**, especially if the model is working with long contexts."
   ],
   "metadata": {
    "id": "ipcuX4CFjcyc",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Solution — Paged Attention\n",
    "\n",
    "Paged Attention proposes an approach inspired by **memory management principles in operating systems**, where data is divided into fixed-size blocks (pages). The idea is to split large matrices of keys and values into smaller \"pages\" of fixed size and manage them more efficiently."
   ],
   "metadata": {
    "id": "AcXHjRWsjePy",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Key Ideas Behind Paged Attention:\n",
    "\n",
    "1. **Splitting into Pages:**\n",
    "   - The matrices of keys and values are divided into small blocks (pages) of fixed size.\n",
    "   - For example, if the sequence length N=1024 and the page size P=64, the matrix will be split into 1024/64=16 pages.\n",
    "2. **Memory Management:**\n",
    "   - Instead of storing the entire matrix in RAM, only active pages (those needed for current computations) are loaded into fast memory (e.g., GPU RAM).\n",
    "   - Inactive pages can be moved to slower memory (e.g., CPU RAM or even disk).\n",
    "3. **Optimized Access:**\n",
    "   - When the model processes a specific position in the sequence, it loads only the pages that contain the corresponding keys and values.\n",
    "   - This significantly reduces memory usage and avoids overloading GPU RAM.\n",
    "4. **Support for Long Sequences:**\n",
    "   - Thanks to paging, the model can handle sequences that far exceed the available memory of the device."
   ],
   "metadata": {
    "id": "kA4LqPYchq6K",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Practice"
   ],
   "metadata": {
    "id": "drThV-YqzqQv",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Toy implementation of Paged Attention."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PagedAttention:\n",
    "    def __init__(self, page_size=128):\n",
    "        self.page_size = page_size\n",
    "        self.pages = {}\n",
    "\n",
    "    def add_to_page(self, key, value, page_id):\n",
    "        if page_id not in self.pages:\n",
    "            self.pages[page_id] = {\"keys\": [], \"values\": []}\n",
    "        self.pages[page_id][\"keys\"].append(key)\n",
    "        self.pages[page_id][\"values\"].append(value)\n",
    "\n",
    "    def get_attention(self, query, page_id):\n",
    "        if page_id not in self.pages:\n",
    "            raise ValueError(\"Page not found!\")\n",
    "\n",
    "        keys = torch.stack(self.pages[page_id][\"keys\"])\n",
    "        values = torch.stack(self.pages[page_id][\"values\"])\n",
    "        attention_scores = torch.matmul(query, keys.transpose(-2, -1))\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output"
   ],
   "metadata": {
    "id": "RZlK0XSPuapS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "paged_attention = PagedAttention(page_size=128)\n",
    "\n",
    "# q,k,v examples\n",
    "query = torch.randn(1, 64)\n",
    "key = torch.randn(1, 64)\n",
    "value = torch.randn(1, 64)\n",
    "\n",
    "paged_attention.add_to_page(key, value, page_id=0)\n",
    "output = paged_attention.get_attention(query, page_id=0)\n",
    "print(f\"Output attention: {output}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgGV6ADQucWw",
    "outputId": "928a7699-8651-4936-8c97-ed422b5dce34",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output attention: tensor([[[ 0.1459,  0.1059, -2.0250,  1.4762,  0.1633, -1.2413,  0.1044,\n",
      "          -0.1903,  0.2401, -0.7890, -1.4566,  0.2000, -0.5124,  0.1269,\n",
      "          -0.8701, -1.1005,  0.0757, -1.0219, -1.2644,  1.2178, -3.2895,\n",
      "          -1.3653,  2.0364, -1.4599, -0.2574, -0.0435, -0.2313, -0.2819,\n",
      "          -0.4846,  1.3289, -1.7133,  0.0908, -0.9375, -1.4625, -0.3167,\n",
      "          -0.3357,  1.1040, -0.7233, -0.1881, -0.4465,  1.2115,  0.0487,\n",
      "          -0.4799, -0.7031, -1.8934,  1.1073,  0.8395, -0.7706, -2.1159,\n",
      "           0.1847, -2.5267, -1.0058, -0.6559, -0.3694,  1.4340, -0.0253,\n",
      "           2.2207, -1.1776, -0.6141,  0.3837,  0.9933,  0.9670,  0.6336,\n",
      "          -0.6800]]])\n"
     ]
    }
   ]
  }
 ]
}