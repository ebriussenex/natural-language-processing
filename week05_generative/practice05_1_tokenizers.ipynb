{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Intro to the Tokenizers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "QbIM4ULpm9R-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenization is the process of splitting text into smaller units (tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "38OU9QV-m9SA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = https://raw.githubusercontent.com/lwtztea/ml_pic/9211f0d/week_5/tokenization_pipeline.png width = 1000 >"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "L0HgMYaNm9SD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "HuggingFace Tokenizers Summary: https://huggingface.co/docs/transformers/tokenizer_summary"
   ],
   "metadata": {
    "id": "xXV2FWazTM0e",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Word Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "BTdyAOizm9SD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dqLFT7k1m9SC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens: ['Hello', 'world!', 'This', 'is', 'a', 'simple', 'example.']\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"Hello world! This is a simple example.\"\n",
    "tokenized_text = TEXT.split()\n",
    "print(f\"Word tokens: {tokenized_text}\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OvzvUtzm9SE",
    "outputId": "6682054e-bd5a-4bbf-df74-1cb90dd7a8bc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Limitations:**\n",
    "\n",
    "* Splits on whitespace, handles punctuation poorly\n",
    "* Can't handle out-of-vocabulary words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "jE4PbD7fm9SE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Byte-Pair Encoding\n",
    "\n",
    "https://arxiv.org/pdf/1508.07909\n",
    "\n",
    "BPE is used in models like RoBERTa and GPT-2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "dJbNoRism9SF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Algorithm Steps:**\n",
    "\n",
    "1. Split text into characters\n",
    "2. Count adjacent pair frequencies\n",
    "3. Merge most frequent pair\n",
    "4. Repeat for desired vocabulary size\n",
    "\n",
    "Final vocab is composed of the special token, the initial alphabet, and all the results of the merges.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "LHNgGLSJm9SF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stanford detailed video about BPE: https://www.youtube.com/watch?v=tOMjTCO0htA\n",
    "\n",
    "HuggingFace BPE explanation: https://www.youtube.com/watch?v=HEikzVL-lZU"
   ],
   "metadata": {
    "id": "tr_frRaUOZBp",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = https://raw.githubusercontent.com/lwtztea/ml_pic/611cde3/week_5/bpe_tokenization.png width = 1000 >"
   ],
   "metadata": {
    "id": "Qx_9ek_8O8id",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementing BPE from Scratch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "tkncakZUm9SG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary: {'d', 'i', 'g', 'x', 'y', 'm', 'f', 'u', 'z', 'e', 'q', 't', 'n', 'a', 'v', 'c', 's', 'h', 'l', 'o', 'p', 'b', 'j', 'r'}\n",
      "Initial vocabulary size: 24\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "CORPUS = \"\"\"\n",
    "Split text into characters\n",
    "Count adjacent pair frequencies\n",
    "Merge most frequent pair\n",
    "Repeat for desired vocabulary size\n",
    "\"\"\"\n",
    "tokenized_words = [list(word) for word in preprocess(CORPUS)]  # basic (char-level) tokenization\n",
    "initial_vocab = {char for word in tokenized_words for char in word}\n",
    "\n",
    "print(f\"Initial vocabulary: {initial_vocab}\")\n",
    "print(f\"Initial vocabulary size: {len(initial_vocab)}\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCLJ3eium9SG",
    "outputId": "26415785-5faa-4e60-8dc6-bd297d2e74f2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['s p l i t',\n 't e x t',\n 'i n t o',\n 'c h a r a c t e r s',\n 'c o u n t',\n 'a d j a c e n t',\n 'p a i r',\n 'f r e q u e n c i e s',\n 'm e r g e',\n 'm o s t',\n 'f r e q u e n t',\n 'p a i r',\n 'r e p e a t',\n 'f o r',\n 'd e s i r e d',\n 'v o c a b u l a r y',\n 's i z e']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_words\n",
    "[\" \".join(word) for word in tokenized_words]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5C81UKicm9SH",
    "outputId": "d2266d02-3bfa-4686-b1fd-09865dbfac5a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def merge_tokens(pair, corpus_in):\n",
    "    corpus_out = []\n",
    "    bigram = re.escape(\" \".join(pair))\n",
    "    p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "    for word in corpus_in:\n",
    "        new_word = p.sub(\"\".join(pair), word)\n",
    "        corpus_out.append(new_word)\n",
    "    return corpus_out\n",
    "\n",
    "\n",
    "def learn_bpe(words, num_merges):\n",
    "    tokenized_corpus = [\" \".join(word) for word in words]\n",
    "    vocab = {char for word in words for char in word}\n",
    "    merges = []\n",
    "\n",
    "    for _ in range(num_merges):\n",
    "        pairs = defaultdict(int)\n",
    "        for word in tokenized_corpus:\n",
    "            token = word.split()\n",
    "            for j in range(len(token) - 1):\n",
    "                pairs[(token[j], token[j + 1])] += 1\n",
    "\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        best_pair = max(pairs, key=pairs.get)  # most common pair\n",
    "        merges.append(best_pair)\n",
    "        vocab.add(best_pair[0] + best_pair[1])\n",
    "\n",
    "        tokenized_corpus = merge_tokens(best_pair, tokenized_corpus)\n",
    "\n",
    "    return merges, vocab"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "i9JBOZwKm9SH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned merges:\n",
      "('n', 't')\n",
      "('r', 'e')\n",
      "('t', 'e')\n",
      "('a', 'r')\n",
      "('a', 'c')\n",
      "('e', 'nt')\n",
      "('p', 'a')\n",
      "('pa', 'i')\n",
      "('pai', 'r')\n",
      "('f', 're')\n",
      "('fre', 'q')\n",
      "('freq', 'u')\n",
      "('e', 's')\n",
      "('s', 'p')\n",
      "('sp', 'l')\n",
      "('spl', 'i')\n",
      "('spli', 't')\n",
      "('te', 'x')\n",
      "('tex', 't')\n",
      "('i', 'nt')\n",
      "('int', 'o')\n",
      "('c', 'h')\n",
      "('ch', 'ar')\n",
      "('char', 'ac')\n",
      "('charac', 'te')\n",
      "('characte', 'r')\n",
      "('character', 's')\n",
      "('c', 'o')\n",
      "('co', 'u')\n",
      "('cou', 'nt')\n",
      "('a', 'd')\n",
      "('ad', 'j')\n",
      "('adj', 'ac')\n",
      "('adjac', 'ent')\n",
      "('frequ', 'e')\n",
      "('freque', 'n')\n",
      "('frequen', 'c')\n",
      "('frequenc', 'i')\n",
      "('frequenci', 'es')\n",
      "('m', 'e')\n",
      "('me', 'r')\n",
      "('mer', 'g')\n",
      "('merg', 'e')\n",
      "('m', 'o')\n",
      "('mo', 's')\n",
      "('mos', 't')\n",
      "('frequ', 'ent')\n",
      "('re', 'p')\n",
      "('rep', 'e')\n",
      "('repe', 'a')\n"
     ]
    }
   ],
   "source": [
    "learned_merges, new_vocab = learn_bpe(tokenized_words, 50)\n",
    "print(\"Learned merges:\", *learned_merges, sep=\"\\n\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ajdVx9em9SH",
    "outputId": "c8998b1d-4ee1-4762-87f0-3bab653ac563"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vocabulary: {'characters', 'frequent', 'd', 'i', 'text', 'g', 'pa', 'cou', 'tex', 'char', 'count', 'adj', 'repea', 'x', 'ar', 'y', 'repe', 'm', 'fre', 'f', 'most', 'es', 'u', 'z', 're', 'e', 'merge', 'te', 'frequ', 'q', 't', 'pair', 'int', 'n', 'a', 'adjac', 'v', 'c', 'frequen', 'mer', 's', 'merg', 'rep', 'split', 'charac', 'spl', 'characte', 'ad', 'sp', 'frequencies', 'mos', 'spli', 'h', 'frequenc', 'l', 'pai', 'nt', 'into', 'o', 'ent', 'me', 'p', 'ac', 'freque', 'frequenci', 'b', 'adjacent', 'mo', 'j', 'ch', 'co', 'r', 'freq', 'character'}\n",
      "Vocabulary size: 74\n"
     ]
    }
   ],
   "source": [
    "print(f\"New vocabulary: {new_vocab}\")\n",
    "print(f\"Vocabulary size: {len(new_vocab)}\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GArr_7Tmm9SH",
    "outputId": "06966049-642a-4532-e551-271b837a604f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 'adjaces adjacent': ['adjac', 'es', ' ', 'adjacent']\n"
     ]
    }
   ],
   "source": [
    "def bpe_encode(word, merges):\n",
    "    encoded = list(word.lower())\n",
    "    for pair in merges:\n",
    "        i = 0\n",
    "        while i < len(encoded) - 1:\n",
    "            if (encoded[i], encoded[i + 1]) == pair:\n",
    "                encoded = encoded[:i] + [pair[0] + pair[1]] + encoded[i + 2:]\n",
    "            else:\n",
    "                i += 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "TEXT = \"adjaces adjacent\"\n",
    "print(f\"Encoded '{TEXT}': {bpe_encode(TEXT, learned_merges)}\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2kLIMM9m9SH",
    "outputId": "6aae8dd5-9545-4d55-86ec-b28c682b1f18"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WordPiece"
   ],
   "metadata": {
    "id": "eXJ1rFAGYYeL",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = https://raw.githubusercontent.com/lwtztea/ml_pic/611cde3/week_5/wordpiece_tokenization.png width = 1000 >"
   ],
   "metadata": {
    "id": "GsolRodKSBsH",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "WordPiece is a subword tokenization algorithm used in models like BERT. Key features:\n",
    "- Uses a probabilistic approach to build vocabulary\n",
    "- Balances between character-level and word-level tokenization\n",
    "- Adds special suffix (e.g., \"##\") to denote subword units"
   ],
   "metadata": {
    "id": "jwZb6eymYeBQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Algorithm Steps:**\n",
    "\n",
    "1. Each word is split by adding \"##\" prefix to all the characters inside it\n",
    "2. Initial alphabet contains all the characters present at the beginning of the words and the characters present inside the words\n",
    "3. Iteratively merge most probable symbol pairs to maximize language model likelihood\n",
    "\n",
    "$$\\text{pair_score} = \\frac{\\text{freq_of_pair}}{\\text{freq_of_first_element } \\times \\text{ freq_of_second_element}}$$"
   ],
   "metadata": {
    "id": "1mjLhoNpYiWU",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class WordPieceTokenizer:\n",
    "    def __init__(self, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
    "        \"\"\"\n",
    "        Initialize the WordPiece tokenizer.\n",
    "\n",
    "        Args:\n",
    "            unk_token (str): The token to represent unknown words.\n",
    "            max_input_chars_per_word (int): Maximum length of a word to tokenize.\n",
    "        \"\"\"\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "        self.vocab = None\n",
    "        self.word_piece_regex = re.compile(r\"[\\w]+[']?[\\w]*\")\n",
    "\n",
    "    def learn_vocab(self, text, vocab_size=1000):\n",
    "        \"\"\"\n",
    "        Learn the vocabulary from the provided text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to learn the vocabulary from.\n",
    "            vocab_size (int): The desired size of the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initialize vocabulary with individual characters and special tokens\n",
    "        self.vocab = {\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}\n",
    "        words = self.word_piece_regex.findall(text.lower())  # Convert to lowercase for consistency\n",
    "        word_counts = Counter(words)\n",
    "\n",
    "        # Add individual characters to the vocabulary\n",
    "        for word in word_counts:\n",
    "            for char in word:\n",
    "                self.vocab.add(char)\n",
    "\n",
    "        # Perform WordPiece training\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            # Count pairs of subwords\n",
    "            pair_counts = defaultdict(int)\n",
    "            for word, count in word_counts.items():\n",
    "                subwords = self._split_to_current_subwords(word)\n",
    "                if len(subwords) <= 1:\n",
    "                    continue\n",
    "\n",
    "                for i in range(len(subwords) - 1):\n",
    "                    pair = (subwords[i], subwords[i + 1])\n",
    "                    pair_counts[pair] += count\n",
    "\n",
    "            if not pair_counts:\n",
    "                break\n",
    "\n",
    "            # Find the most frequent pair\n",
    "            best_pair = max(pair_counts.items(), key=lambda x: x[1], default=(None, 0))\n",
    "            if best_pair[0] is None:\n",
    "                break\n",
    "\n",
    "            first, second = best_pair[0]\n",
    "            # Create the new subword\n",
    "            if second.startswith(\"##\"):\n",
    "                new_subword = first + second[2:]\n",
    "            else:\n",
    "                new_subword = first + second\n",
    "\n",
    "            # Add the new subword to the vocabulary\n",
    "            self.vocab.add(new_subword)\n",
    "\n",
    "            # No need to update word_counts as we're only building the vocabulary\n",
    "\n",
    "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Sample vocabulary items: {list(self.vocab)[:30]}\")\n",
    "\n",
    "    def _split_to_current_subwords(self, word):\n",
    "        \"\"\"\n",
    "        Split a word into its current subwords based on the vocabulary.\n",
    "        This is a helper function for learning the vocabulary.\n",
    "        \"\"\"\n",
    "        subwords = []\n",
    "        start = 0\n",
    "        while start < len(word):\n",
    "            end = len(word)\n",
    "            curr_subword = None\n",
    "\n",
    "            while start < end:\n",
    "                substring = word[start:end]\n",
    "                if start > 0:\n",
    "                    substring = \"##\" + substring\n",
    "\n",
    "                if substring in self.vocab:\n",
    "                    curr_subword = substring\n",
    "                    break\n",
    "                end -= 1\n",
    "\n",
    "            if curr_subword is None:\n",
    "                # If no subword is found, use the first character and add prefix if needed\n",
    "                end = start + 1\n",
    "                substring = word[start:end]\n",
    "                if start > 0:\n",
    "                    substring = \"##\" + substring\n",
    "                curr_subword = substring\n",
    "\n",
    "            subwords.append(curr_subword)\n",
    "            start = end\n",
    "\n",
    "        return subwords\n",
    "\n",
    "    def _word_to_subwords(self, word):\n",
    "        \"\"\"\n",
    "        Split a word into subwords using the current vocabulary.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of subwords.\n",
    "        \"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "\n",
    "        subwords = []\n",
    "        start = 0\n",
    "        while start < len(word):\n",
    "            end = len(word)\n",
    "            found = False\n",
    "\n",
    "            while start < end:\n",
    "                substring = word[start:end]\n",
    "                if start > 0:\n",
    "                    substring = \"##\" + substring\n",
    "\n",
    "                if substring in self.vocab:\n",
    "                    subwords.append(substring)\n",
    "                    start = end\n",
    "                    found = True\n",
    "                    break\n",
    "                end -= 1\n",
    "\n",
    "            if not found:\n",
    "                # If no subword is found, use the unknown token and move on\n",
    "                subwords.append(self.unk_token)\n",
    "                break\n",
    "\n",
    "        return subwords\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the input text into subwords.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tokens.\n",
    "        \"\"\"\n",
    "        if self.vocab is None:\n",
    "            raise ValueError(\n",
    "                \"Vocabulary not learned. Call `learn_vocab` first.\")\n",
    "\n",
    "        tokens = []\n",
    "        for word in self.word_piece_regex.findall(text.lower()):\n",
    "            if len(word) > self.max_input_chars_per_word:\n",
    "                tokens.append(self.unk_token)\n",
    "                continue\n",
    "            subwords = self._word_to_subwords(word)\n",
    "            tokens.extend(subwords)\n",
    "        return tokens"
   ],
   "metadata": {
    "id": "FdJpC7aETVRg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TEXT = \"hello world! This is a test. WordPiece is awesome.\"\n",
    "\n",
    "tokenizer = WordPieceTokenizer()\n",
    "\n",
    "# Learn the vocabulary\n",
    "tokenizer.learn_vocab(TEXT, vocab_size=40)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcsq2VoITcGY",
    "outputId": "a1f0239d-3b4b-41a2-d42e-dfbd50fd5712",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 40\n",
      "Sample vocabulary items: ['wordpiec', 'hello', 'wordpi', 'wo', 'm', 'wordp', 'worl', 'hell', '[MASK]', 'tes', 'wordpiece', 'c', '[PAD]', 'thi', 'word', 'he', 'r', 'this', 'is', 'd', 'i', '[CLS]', 'hel', '##es', 'e', 't', 'a', 'world', 's', 'w']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "NEW_TEXT = \"hello world! This is a new test.\"\n",
    "tokens = tokenizer.tokenize(NEW_TEXT)\n",
    "print(\"Tokens:\", tokens)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0cUt2RDTiZm",
    "outputId": "9e1a6586-a780-4752-e723-305fccab1eea",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', 'world', 'this', 'is', 'a', '[UNK]', 'test']\n"
     ]
    }
   ]
  }
 ]
}